{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "49344c72-d0ea-4092-96fe-ed5508ae6e0b",
    "_uuid": "20b4762eb8607ed428703c2156c5aefe8b49ff3f"
   },
   "source": [
    "<a id=\"3\"></a> <br>\n",
    "### Logistic Regression\n",
    "- Linear regression is not good at classification.\n",
    "- We use logistic regression for classification.\n",
    "- linear regression + logistic function(softmax) = logistic regression\n",
    "- Check my deep learning tutorial. There is detailed explanation of logistic regression. \n",
    "    - https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners\n",
    "- **Steps of Logistic Regression**\n",
    "    1. Import Libraries\n",
    "    1. Prepare Dataset\n",
    "        - We use MNIST dataset.\n",
    "        - There are 28*28 images and 10 labels from 0 to 9\n",
    "        - Data is not normalized so we divide each image to 255 that is basic normalization for images.\n",
    "        - In order to split data, we use train_test_split method from sklearn library\n",
    "        - Size of train data is 80% and size of test data is 20%.\n",
    "        - Create feature and target tensors. At the next parts we create variable from these tensors. As you remember we need to define variable for accumulation of gradients.\n",
    "        - batch_size = batch size means is that for example we have data and it includes 1000 sample. We can train 1000 sample in a same time or we can divide it 10 groups which include 100 sample and train 10 groups in order. Batch size is the group size. For example, I choose batch_size = 100, that means in order to train all data only once we have 336 groups. We train each groups(336) that have batch_size(quota) 100. Finally we train 33600 sample one time.\n",
    "        - epoch: 1 epoch means training all samples one time.\n",
    "        - In our example: we have 33600 sample to train and we decide our batch_size is 100. Also we decide epoch is 29(accuracy achieves almost highest value when epoch is 29). Data is trained 29 times. Question is that how many iteration do I need? Lets calculate: \n",
    "            - training data 1 times = training 33600 sample (because data includes 33600 sample) \n",
    "            - But we split our data 336 groups(group_size = batch_size = 100) our data \n",
    "            - Therefore, 1 epoch(training data only once) takes 336 iteration\n",
    "            - We have 29 epoch, so total iterarion is 9744(that is almost 10000 which I used)\n",
    "        - TensorDataset(): Data set wrapping tensors. Each sample is retrieved by indexing tensors along the first dimension.\n",
    "        - DataLoader(): It combines dataset and sample. It also provides multi process iterators over the dataset.\n",
    "        - Visualize one of the images in dataset\n",
    "    1. Create Logistic Regression Model\n",
    "        - Same with linear regression.\n",
    "        - However as you expect, there should be logistic function in model right?\n",
    "        - In pytorch, logistic function is in the loss function where we will use at next parts.\n",
    "    1. Instantiate Model\n",
    "        - input_dim = 28*28 # size of image px*px\n",
    "        - output_dim = 10  # labels 0,1,2,3,4,5,6,7,8,9\n",
    "        - create model\n",
    "    1. Instantiate Loss \n",
    "        - Cross entropy loss\n",
    "        - It calculates loss that is not surprise :)\n",
    "        - It also has softmax(logistic function) in it.\n",
    "    1. Instantiate Optimizer \n",
    "        - SGD Optimizer\n",
    "    1. Traning the Model\n",
    "    1. Prediction\n",
    "- As a result, as you can see from plot, while loss decreasing, accuracy(almost 85%) is increasing and our model is learning(training).    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "a0bf0fa7-c527-4fd3-b504-02a88fc94798",
    "_uuid": "1382c63fe24710d3b2840e7dcf172cddbf533743"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "59cdc9d5-da8f-4d7a-abc5-c62b0008afb0",
    "_uuid": "c6e0d7d3843719091564a580dbe08f67ee0d93ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 20\n",
    "\n",
    "# Transform: Normalize and Flatten Images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(-1))  # Flatten the image\n",
    "])\n",
    "\n",
    "# Load MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "03a25584-c567-4b5e-bae1-7bc9e02184fe",
    "_uuid": "7c7a7265a23a8101d5ed0c8826dfec3726d6161d"
   },
   "outputs": [],
   "source": [
    "# Create Logistic Regression Model\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        # Linear part\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        # There should be logistic function right?\n",
    "        # However logistic function in pytorch is in loss function\n",
    "        # So actually we do not forget to put it, it is only at next parts\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "# Instantiate Model Class\n",
    "input_dim = 28*28 # size of image px*px\n",
    "output_dim = 10  # labels 0,1,2,3,4,5,6,7,8,9\n",
    "\n",
    "# create logistic regression model\n",
    "model = LogisticRegressionModel(input_dim, output_dim).to(device)\n",
    "\n",
    "# Cross Entropy Loss  \n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD Optimizer \n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "82de08d9-7f3c-4eb9-8a99-9d7a8677799c",
    "_uuid": "0cab9c3ec72f73db1b06578fa7a51611141e16da"
   },
   "outputs": [],
   "source": [
    "# Traning the Model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for (images, labels) in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = error(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "        \n",
    "    # Calculate Accuracy         \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Predict test dataset\n",
    "    for images, labels in test_loader: \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        predicted = torch.max(outputs.data, 1)[1]\n",
    "        total += len(labels)\n",
    "        correct += (predicted == labels).sum()\n",
    "    \n",
    "    accuracy = 100 * correct / float(total)\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}], Loss: {avg_loss:.4f}, Acc: {accuracy:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "924e9606-e155-4e39-89d3-39f941fd52f8",
    "_uuid": "db87c03e9d263f07eb75f82a914d3e966895a6c1"
   },
   "outputs": [],
   "source": [
    "# visualization\n",
    "# plt.plot(iteration_list,loss_list)\n",
    "# plt.xlabel(\"Number of iteration\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"Logistic Regression: Loss vs Number of iteration\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4d38db05-fad0-468c-9000-20caf5465eca",
    "_uuid": "ea9eba414f2f0f1e63ef564dc0ee708c753ff51f"
   },
   "source": [
    "<a id=\"4\"></a> <br>\n",
    "### Artificial Neural Network (ANN)\n",
    "- Logistic regression is good at classification but when complexity(non linearity) increases, the accuracy of model decreases.\n",
    "- Therefore, we need to increase complexity of model.\n",
    "- In order to increase complexity of model, we need to add more non linear functions as hidden layer. \n",
    "- I am saying again that if you do not know what is artificial neural network check my deep learning tutorial because I will not explain neural network detailed here, only explain pytorch.\n",
    "- Artificial Neural Network tutorial: https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners\n",
    "- What we expect from artificial neural network is that when complexity increases, we use more hidden layers and our model can adapt better. As a result accuracy increase.\n",
    "- **Steps of ANN:**\n",
    "    1. Import Libraries\n",
    "        - In order to show you, I import again but we actually imported them at previous parts.\n",
    "    1. Prepare Dataset\n",
    "        - Totally same with previous part(logistic regression).\n",
    "        - We use same dataset so we only need train_loader and test_loader. \n",
    "        - We use same batch size, epoch and iteration numbers.\n",
    "    1. Create ANN Model\n",
    "        - We add 3 hidden layers.\n",
    "        - We use ReLU, Tanh and ELU activation functions for diversity.\n",
    "    1. Instantiate Model Class\n",
    "        - input_dim = 28*28 # size of image px*px\n",
    "        - output_dim = 10  # labels 0,1,2,3,4,5,6,7,8,9\n",
    "        - Hidden layer dimension is 150. I only choose it as 150 there is no reason. Actually hidden layer dimension is hyperparameter and it should be chosen and tuned. You can try different values for hidden layer dimension and observe the results.\n",
    "        - create model\n",
    "    1. Instantiate Loss\n",
    "        - Cross entropy loss\n",
    "        - It also has softmax(logistic function) in it.\n",
    "    1. Instantiate Optimizer\n",
    "        - SGD Optimizer\n",
    "    1. Traning the Model\n",
    "    1. Prediction\n",
    "- As a result, as you can see from plot, while loss decreasing, accuracy is increasing and our model is learning(training). \n",
    "- Thanks to hidden layers model learnt better and accuracy(almost 95%) is better than accuracy of logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "3472f1c1-5888-4abe-822c-3a493a5f8be5",
    "_uuid": "cefd0bb2f23b80f30ca65cbb08859ad81ab12e08"
   },
   "outputs": [],
   "source": [
    "# Create ANN Model\n",
    "class ANNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANNModel, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(28 * 28, 128) \n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(32, 10)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = ANNModel().to(device)\n",
    "error = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.02\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "7550e98b-5011-4d09-88ee-97b0ecbc6f19",
    "_uuid": "c91694f3af94e4e1b76ab01489e186718c70ccd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Loss: 0.0007, Acc: 78.9800%\n",
      "Epoch [1/50], Loss: 0.0005, Acc: 88.5000%\n",
      "Epoch [2/50], Loss: 0.0003, Acc: 91.3500%\n",
      "Epoch [3/50], Loss: 0.0002, Acc: 92.5000%\n",
      "Epoch [4/50], Loss: 0.0001, Acc: 93.5700%\n",
      "Epoch [5/50], Loss: 0.0002, Acc: 94.4400%\n",
      "Epoch [6/50], Loss: 0.0001, Acc: 95.0800%\n",
      "Epoch [7/50], Loss: 0.0001, Acc: 95.5800%\n",
      "Epoch [8/50], Loss: 0.0002, Acc: 95.7200%\n",
      "Epoch [9/50], Loss: 0.0002, Acc: 95.9900%\n",
      "Epoch [10/50], Loss: 0.0000, Acc: 96.3400%\n",
      "Epoch [11/50], Loss: 0.0001, Acc: 96.3700%\n",
      "Epoch [12/50], Loss: 0.0006, Acc: 96.8500%\n",
      "Epoch [13/50], Loss: 0.0000, Acc: 97.1100%\n",
      "Epoch [14/50], Loss: 0.0000, Acc: 96.7500%\n",
      "Epoch [15/50], Loss: 0.0001, Acc: 96.8700%\n",
      "Epoch [16/50], Loss: 0.0000, Acc: 97.3300%\n",
      "Epoch [17/50], Loss: 0.0000, Acc: 97.2600%\n",
      "Epoch [18/50], Loss: 0.0000, Acc: 97.4000%\n",
      "Epoch [19/50], Loss: 0.0000, Acc: 97.2400%\n",
      "Epoch [20/50], Loss: 0.0001, Acc: 97.2200%\n",
      "Epoch [21/50], Loss: 0.0000, Acc: 97.4500%\n",
      "Epoch [22/50], Loss: 0.0000, Acc: 97.3300%\n",
      "Epoch [23/50], Loss: 0.0000, Acc: 97.5400%\n",
      "Epoch [24/50], Loss: 0.0000, Acc: 97.3600%\n",
      "Epoch [25/50], Loss: 0.0000, Acc: 97.5300%\n",
      "Epoch [26/50], Loss: 0.0000, Acc: 97.5200%\n",
      "Epoch [27/50], Loss: 0.0000, Acc: 97.5200%\n",
      "Epoch [28/50], Loss: 0.0000, Acc: 97.3000%\n",
      "Epoch [29/50], Loss: 0.0000, Acc: 97.5700%\n",
      "Epoch [30/50], Loss: 0.0001, Acc: 97.6000%\n",
      "Epoch [31/50], Loss: 0.0000, Acc: 97.7700%\n",
      "Epoch [32/50], Loss: 0.0000, Acc: 97.6100%\n",
      "Epoch [33/50], Loss: 0.0000, Acc: 97.6600%\n",
      "Epoch [34/50], Loss: 0.0000, Acc: 97.5300%\n",
      "Epoch [35/50], Loss: 0.0001, Acc: 97.2000%\n",
      "Epoch [36/50], Loss: 0.0000, Acc: 97.5800%\n",
      "Epoch [37/50], Loss: 0.0000, Acc: 97.6700%\n",
      "Epoch [38/50], Loss: 0.0000, Acc: 97.6200%\n",
      "Epoch [39/50], Loss: 0.0000, Acc: 97.5800%\n",
      "Epoch [40/50], Loss: 0.0000, Acc: 97.6900%\n",
      "Epoch [41/50], Loss: 0.0000, Acc: 97.6800%\n",
      "Epoch [42/50], Loss: 0.0000, Acc: 97.6000%\n",
      "Epoch [43/50], Loss: 0.0000, Acc: 97.5600%\n",
      "Epoch [44/50], Loss: 0.0000, Acc: 97.6900%\n",
      "Epoch [45/50], Loss: 0.0000, Acc: 97.6300%\n",
      "Epoch [46/50], Loss: 0.0000, Acc: 97.6600%\n",
      "Epoch [47/50], Loss: 0.0000, Acc: 97.7300%\n",
      "Epoch [48/50], Loss: 0.0000, Acc: 97.7600%\n",
      "Epoch [49/50], Loss: 0.0000, Acc: 97.7900%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "# Traning the Model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for (images, labels) in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = error(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "        \n",
    "    # Calculate Accuracy         \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Predict test dataset\n",
    "    for images, labels in test_loader: \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        predicted = torch.max(outputs.data, 1)[1]\n",
    "        total += len(labels)\n",
    "        correct += (predicted == labels).sum()\n",
    "    \n",
    "    accuracy = 100 * correct / float(total)\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}], Loss: {avg_loss:.4f}, Acc: {accuracy:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "5579a7d6-7766-4d0f-b9d0-584cb4f28321",
    "_uuid": "c5e2e6da7f1ee801e38358dc28d4c99e32d2b761"
   },
   "outputs": [],
   "source": [
    "# # visualization loss \n",
    "# plt.plot(iteration_list,loss_list)\n",
    "# plt.xlabel(\"Number of iteration\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"ANN: Loss vs Number of iteration\")\n",
    "# plt.show()\n",
    "\n",
    "# # visualization accuracy \n",
    "# plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
    "# plt.xlabel(\"Number of iteration\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.title(\"ANN: Accuracy vs Number of iteration\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "50bbb2e7-15d8-47f5-8f31-25e0d0cb9e29",
    "_uuid": "cd8f261d231acaccd0f0bc8466fc28c1b0c2f567"
   },
   "source": [
    "<a id=\"5\"></a> <br>\n",
    "### Convolutional Neural Network (CNN)\n",
    "- CNN is well adapted to classify images.\n",
    "- You can learn CNN basics: https://www.kaggle.com/kanncaa1/convolutional-neural-network-cnn-tutorial\n",
    "- **Steps of CNN:**\n",
    "    1. Import Libraries\n",
    "    1. Prepare Dataset\n",
    "        - Totally same with previous parts.\n",
    "        - We use same dataset so we only need train_loader and test_loader. \n",
    "    1. Convolutional layer: \n",
    "        - Create feature maps with filters(kernels).\n",
    "        - Padding: After applying filter, dimensions of original image decreases. However, we want to preserve as much as information about the original image. We can apply padding to increase dimension of feature map after convolutional layer.\n",
    "        - We use 2 convolutional layer.\n",
    "        - Number of feature map is out_channels = 16\n",
    "        - Filter(kernel) size is 5*5\n",
    "    1. Pooling layer: \n",
    "        - Prepares a condensed feature map from output of convolutional layer(feature map) \n",
    "        - 2 pooling layer that we will use max pooling.\n",
    "        - Pooling size is 2*2\n",
    "    1. Flattening: Flats the features map\n",
    "    1. Fully Connected Layer: \n",
    "        - Artificial Neural Network that we learnt at previous part.\n",
    "        - Or it can be only linear like logistic regression but at the end there is always softmax function.\n",
    "        - We will not use activation function in fully connected layer.\n",
    "        - You can think that our fully connected layer is logistic regression.\n",
    "        - We combine convolutional part and logistic regression to create our CNN model.\n",
    "    1. Instantiate Model Class\n",
    "        - create model\n",
    "    1. Instantiate Loss\n",
    "        - Cross entropy loss\n",
    "        - It also has softmax(logistic function) in it.\n",
    "    1. Instantiate Optimizer\n",
    "        - SGD Optimizer\n",
    "    1. Traning the Model\n",
    "    1. Prediction\n",
    "- As a result, as you can see from plot, while loss decreasing, accuracy is increasing and our model is learning(training). \n",
    "- Thanks to convolutional layer, model learnt better and accuracy(almost 98%) is better than accuracy of ANN. Actually while tuning hyperparameters, increase in iteration and expanding convolutional neural network can increase accuracy but it takes too much running time that we do not want at kaggle.   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "9ca5af9e-6821-4d60-8084-edb523a39c6b",
    "_uuid": "4915535771ffdd33ef480200393216f215b4fc48"
   },
   "outputs": [],
   "source": [
    "# Create CNN Model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 10) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.cnn1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.maxpool1(out)\n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.maxpool2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = CNNModel().to(device)\n",
    "error = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.02\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "99a8903c-da15-496c-96b7-f5402c8fc5f0",
    "_uuid": "f44e02d25698ac1a014795d972a384a3f3003d35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Loss: 0.0000, Acc: 95.3700%\n",
      "Epoch [1/50], Loss: 0.0001, Acc: 96.3400%\n",
      "Epoch [2/50], Loss: 0.0001, Acc: 97.4300%\n",
      "Epoch [3/50], Loss: 0.0000, Acc: 97.9900%\n",
      "Epoch [4/50], Loss: 0.0001, Acc: 97.5200%\n",
      "Epoch [5/50], Loss: 0.0000, Acc: 98.0100%\n",
      "Epoch [6/50], Loss: 0.0000, Acc: 98.2500%\n",
      "Epoch [7/50], Loss: 0.0000, Acc: 98.2900%\n",
      "Epoch [8/50], Loss: 0.0000, Acc: 98.4300%\n",
      "Epoch [9/50], Loss: 0.0001, Acc: 97.9200%\n",
      "Epoch [10/50], Loss: 0.0000, Acc: 98.5000%\n",
      "Epoch [11/50], Loss: 0.0001, Acc: 98.4400%\n",
      "Epoch [12/50], Loss: 0.0000, Acc: 98.4200%\n",
      "Epoch [13/50], Loss: 0.0001, Acc: 98.3900%\n",
      "Epoch [14/50], Loss: 0.0001, Acc: 98.5400%\n",
      "Epoch [15/50], Loss: 0.0000, Acc: 98.6100%\n",
      "Epoch [16/50], Loss: 0.0001, Acc: 98.5600%\n",
      "Epoch [17/50], Loss: 0.0000, Acc: 98.6400%\n",
      "Epoch [18/50], Loss: 0.0000, Acc: 98.6800%\n",
      "Epoch [19/50], Loss: 0.0000, Acc: 98.7300%\n",
      "Epoch [20/50], Loss: 0.0000, Acc: 98.6900%\n",
      "Epoch [21/50], Loss: 0.0000, Acc: 98.8000%\n",
      "Epoch [22/50], Loss: 0.0000, Acc: 98.6800%\n",
      "Epoch [23/50], Loss: 0.0000, Acc: 98.6000%\n",
      "Epoch [24/50], Loss: 0.0000, Acc: 98.7600%\n",
      "Epoch [25/50], Loss: 0.0000, Acc: 98.8100%\n",
      "Epoch [26/50], Loss: 0.0000, Acc: 98.7100%\n",
      "Epoch [27/50], Loss: 0.0000, Acc: 98.7900%\n",
      "Epoch [28/50], Loss: 0.0000, Acc: 98.8100%\n",
      "Epoch [29/50], Loss: 0.0000, Acc: 98.8000%\n",
      "Epoch [30/50], Loss: 0.0000, Acc: 98.7200%\n",
      "Epoch [31/50], Loss: 0.0000, Acc: 98.7700%\n",
      "Epoch [32/50], Loss: 0.0001, Acc: 98.7400%\n",
      "Epoch [33/50], Loss: 0.0000, Acc: 98.6300%\n",
      "Epoch [34/50], Loss: 0.0000, Acc: 98.7500%\n",
      "Epoch [35/50], Loss: 0.0000, Acc: 98.7800%\n",
      "Epoch [36/50], Loss: 0.0000, Acc: 98.8600%\n",
      "Epoch [37/50], Loss: 0.0000, Acc: 98.8700%\n",
      "Epoch [38/50], Loss: 0.0000, Acc: 98.8200%\n",
      "Epoch [39/50], Loss: 0.0000, Acc: 98.9100%\n",
      "Epoch [40/50], Loss: 0.0000, Acc: 98.8300%\n",
      "Epoch [41/50], Loss: 0.0000, Acc: 98.8600%\n",
      "Epoch [42/50], Loss: 0.0000, Acc: 98.8400%\n",
      "Epoch [43/50], Loss: 0.0000, Acc: 98.6400%\n",
      "Epoch [44/50], Loss: 0.0000, Acc: 98.8300%\n",
      "Epoch [45/50], Loss: 0.0000, Acc: 98.7800%\n",
      "Epoch [46/50], Loss: 0.0000, Acc: 98.7800%\n",
      "Epoch [47/50], Loss: 0.0000, Acc: 98.8900%\n",
      "Epoch [48/50], Loss: 0.0000, Acc: 98.8600%\n",
      "Epoch [49/50], Loss: 0.0000, Acc: 98.9300%\n"
     ]
    }
   ],
   "source": [
    "# Traning the Model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for (images, labels) in train_loader:\n",
    "        images = images.to(device).view(-1, 1, 28, 28)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = error(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "        \n",
    "    # Calculate Accuracy         \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Predict test dataset\n",
    "    for images, labels in test_loader: \n",
    "        images = images.to(device).view(-1, 1, 28, 28)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        predicted = torch.max(outputs.data, 1)[1]\n",
    "        total += len(labels)\n",
    "        correct += (predicted == labels).sum()\n",
    "    \n",
    "    accuracy = 100 * correct / float(total)\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}], Loss: {avg_loss:.4f}, Acc: {accuracy:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "ac9e4aee-b8af-4641-8794-bad03b650179",
    "_uuid": "44c1ed412d778f3e6b08f11bddc5321f63e408dd"
   },
   "outputs": [],
   "source": [
    "# # visualization loss \n",
    "# plt.plot(iteration_list,loss_list)\n",
    "# plt.xlabel(\"Number of iteration\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"CNN: Loss vs Number of iteration\")\n",
    "# plt.show()\n",
    "\n",
    "# # visualization accuracy \n",
    "# plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
    "# plt.xlabel(\"Number of iteration\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.title(\"CNN: Accuracy vs Number of iteration\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 861823,
     "sourceId": 3004,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 18199,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "env_playing_with_dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
