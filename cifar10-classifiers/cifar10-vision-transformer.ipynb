{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "a0bf0fa7-c527-4fd3-b504-02a88fc94798",
    "_uuid": "1382c63fe24710d3b2840e7dcf172cddbf533743"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "59cdc9d5-da8f-4d7a-abc5-c62b0008afb0",
    "_uuid": "c6e0d7d3843719091564a580dbe08f67ee0d93ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 20\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(32, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Load CIFAR10 Dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "50bbb2e7-15d8-47f5-8f31-25e0d0cb9e29",
    "_uuid": "cd8f261d231acaccd0f0bc8466fc28c1b0c2f567"
   },
   "source": [
    "<a id=\"5\"></a> <br>\n",
    "### Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "9ca5af9e-6821-4d60-8084-edb523a39c6b",
    "_uuid": "4915535771ffdd33ef480200393216f215b4fc48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT(\n",
      "  (patch_embedding): PatchEmbedding(\n",
      "    (projection): Sequential(\n",
      "      (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=4, p2=4)\n",
      "      (1): Linear(in_features=48, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (transformer_block_1): TransformerBlock(\n",
      "    (multi_head_attn): MultiAttentionHead(\n",
      "      (proj): Linear(in_features=384, out_features=128, bias=True)\n",
      "    )\n",
      "    (mlp): Sequential(\n",
      "      (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (transformer_block_2): TransformerBlock(\n",
      "    (multi_head_attn): MultiAttentionHead(\n",
      "      (proj): Linear(in_features=384, out_features=128, bias=True)\n",
      "    )\n",
      "    (mlp): Sequential(\n",
      "      (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (mlp_head): Sequential(\n",
      "    (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels = 3, patch_size = 4, emb_size = 128):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            # break-down the image in s1 x s2 patches and flat them\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
    "            nn.Linear(patch_size * patch_size * in_channels, emb_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, input_emb_size=128, latent_emb_size=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.wk = nn.Linear(input_emb_size, latent_emb_size)\n",
    "        self.wq = nn.Linear(input_emb_size, latent_emb_size)\n",
    "        self.wv = nn.Linear(input_emb_size, latent_emb_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        k = self.wk(x)\n",
    "        q = self.wq(x)\n",
    "        v = self.wv(x)\n",
    "\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (q.shape[-1] ** 0.5)\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        return torch.matmul(attn_weights, v)\n",
    "\n",
    "\n",
    "class MultiAttentionHead(nn.Module):\n",
    "    def __init__(self, n_head=3, input_emb_dim=128, latent_emb_size=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.multihead = [AttentionHead(input_emb_dim, latent_emb_size) for i in range(n_head)]\n",
    "        [attn_head.to(device) for attn_head in self.multihead]\n",
    "\n",
    "        self.proj = nn.Linear(n_head * latent_emb_size, input_emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        multi_head_output = [attn_head(x) for attn_head in self.multihead]\n",
    "        multi_head_output = torch.cat(multi_head_output, dim=-1)  \n",
    "\n",
    "        return self.proj(multi_head_output)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_head=3, input_emb_dim=128, attn_head_latent_emb_dim=64, mlp_latent_emb_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multi_head_attn = MultiAttentionHead(n_head, input_emb_dim, attn_head_latent_emb_dim)\n",
    "    \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(input_emb_dim),\n",
    "            nn.Linear(input_emb_dim, mlp_latent_emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_latent_emb_dim, input_emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.multi_head_attn(x)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ðŸš€ Fixed Vision Transformer Model for CIFAR-10\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, num_classes=10, \n",
    "                 emb_dim=128):\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
    "        \n",
    "        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n",
    "                                              patch_size=patch_size,\n",
    "                                              emb_size=emb_dim)\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_dim))        \n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, emb_dim))\n",
    "\n",
    "        self.ln = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        self.transformer_block_1 = TransformerBlock(n_head=6, input_emb_dim=emb_dim)\n",
    "        \n",
    "        self.transformer_block_2 = TransformerBlock(n_head=6, input_emb_dim=emb_dim)\n",
    "        \n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(emb_dim),\n",
    "            nn.Linear(emb_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        x = self.patch_embedding(x) \n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  \n",
    "        x = torch.cat((cls_tokens, x), dim=1)  \n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        x = self.ln(x)\n",
    "\n",
    "        dx = self.transformer_block_1(x)\n",
    "        x = x + dx\n",
    "\n",
    "        dx = self.transformer_block_2(x)\n",
    "        x = x + dx\n",
    "\n",
    "        x = self.mlp_head(x)\n",
    "        \n",
    "        x = x[:, 0]  \n",
    "\n",
    "        return x\n",
    "\n",
    "model = ViT().to(device)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 189258\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Example\n",
    "total_params = count_parameters(model)\n",
    "print(f\"Total Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "99a8903c-da15-496c-96b7-f5402c8fc5f0",
    "_uuid": "f44e02d25698ac1a014795d972a384a3f3003d35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.0022, Acc: 30.7300%\n",
      "Epoch [2/100], Loss: 0.0026, Acc: 36.3600%\n",
      "Epoch [3/100], Loss: 0.0024, Acc: 40.3100%\n",
      "Epoch [4/100], Loss: 0.0016, Acc: 44.0200%\n",
      "Epoch [5/100], Loss: 0.0028, Acc: 45.1200%\n",
      "Epoch [6/100], Loss: 0.0014, Acc: 46.8300%\n",
      "Epoch [7/100], Loss: 0.0019, Acc: 47.8700%\n",
      "Epoch [8/100], Loss: 0.0022, Acc: 48.7600%\n",
      "Epoch [9/100], Loss: 0.0019, Acc: 49.1000%\n",
      "Epoch [10/100], Loss: 0.0016, Acc: 49.5400%\n",
      "Epoch [11/100], Loss: 0.0011, Acc: 51.6500%\n",
      "Epoch [12/100], Loss: 0.0011, Acc: 50.6500%\n",
      "Epoch [13/100], Loss: 0.0016, Acc: 51.4000%\n",
      "Epoch [14/100], Loss: 0.0016, Acc: 52.5900%\n",
      "Epoch [15/100], Loss: 0.0019, Acc: 53.1200%\n",
      "Epoch [16/100], Loss: 0.0022, Acc: 52.7600%\n",
      "Epoch [17/100], Loss: 0.0012, Acc: 52.9600%\n",
      "Epoch [18/100], Loss: 0.0018, Acc: 53.4700%\n",
      "Epoch [19/100], Loss: 0.0020, Acc: 53.6000%\n",
      "Epoch [20/100], Loss: 0.0022, Acc: 54.3300%\n",
      "Epoch [21/100], Loss: 0.0016, Acc: 54.2800%\n",
      "Epoch [22/100], Loss: 0.0021, Acc: 54.1300%\n",
      "Epoch [23/100], Loss: 0.0017, Acc: 54.3600%\n",
      "Epoch [24/100], Loss: 0.0022, Acc: 54.7700%\n",
      "Epoch [25/100], Loss: 0.0015, Acc: 54.9100%\n",
      "Epoch [26/100], Loss: 0.0013, Acc: 55.2200%\n",
      "Epoch [27/100], Loss: 0.0012, Acc: 55.0900%\n",
      "Epoch [28/100], Loss: 0.0020, Acc: 55.5700%\n",
      "Epoch [29/100], Loss: 0.0016, Acc: 56.1200%\n",
      "Epoch [30/100], Loss: 0.0015, Acc: 55.9700%\n",
      "Epoch [31/100], Loss: 0.0013, Acc: 56.0200%\n",
      "Epoch [32/100], Loss: 0.0019, Acc: 56.1100%\n",
      "Epoch [33/100], Loss: 0.0012, Acc: 55.5900%\n",
      "Epoch [34/100], Loss: 0.0015, Acc: 56.7500%\n",
      "Epoch [35/100], Loss: 0.0016, Acc: 56.4100%\n",
      "Epoch [36/100], Loss: 0.0017, Acc: 56.5900%\n",
      "Epoch [37/100], Loss: 0.0011, Acc: 56.7000%\n",
      "Epoch [38/100], Loss: 0.0016, Acc: 56.9400%\n",
      "Epoch [39/100], Loss: 0.0013, Acc: 57.5600%\n",
      "Epoch [40/100], Loss: 0.0014, Acc: 57.4400%\n",
      "Epoch [41/100], Loss: 0.0020, Acc: 57.1700%\n",
      "Epoch [42/100], Loss: 0.0015, Acc: 57.7000%\n",
      "Epoch [43/100], Loss: 0.0016, Acc: 56.9200%\n",
      "Epoch [44/100], Loss: 0.0013, Acc: 57.2300%\n",
      "Epoch [45/100], Loss: 0.0016, Acc: 57.9000%\n",
      "Epoch [46/100], Loss: 0.0011, Acc: 58.0900%\n",
      "Epoch [47/100], Loss: 0.0016, Acc: 58.1500%\n",
      "Epoch [48/100], Loss: 0.0017, Acc: 56.9500%\n",
      "Epoch [49/100], Loss: 0.0015, Acc: 57.7400%\n",
      "Epoch [50/100], Loss: 0.0012, Acc: 57.6800%\n",
      "Epoch [51/100], Loss: 0.0012, Acc: 57.8900%\n",
      "Epoch [52/100], Loss: 0.0013, Acc: 57.9000%\n",
      "Epoch [53/100], Loss: 0.0015, Acc: 58.0100%\n",
      "Epoch [54/100], Loss: 0.0015, Acc: 57.3800%\n",
      "Epoch [55/100], Loss: 0.0014, Acc: 58.7300%\n",
      "Epoch [56/100], Loss: 0.0011, Acc: 57.9600%\n",
      "Epoch [57/100], Loss: 0.0021, Acc: 58.0800%\n",
      "Epoch [58/100], Loss: 0.0018, Acc: 58.1700%\n",
      "Epoch [59/100], Loss: 0.0015, Acc: 58.5800%\n",
      "Epoch [60/100], Loss: 0.0012, Acc: 58.3100%\n",
      "Epoch [61/100], Loss: 0.0012, Acc: 58.6500%\n",
      "Epoch [62/100], Loss: 0.0015, Acc: 58.3800%\n",
      "Epoch [63/100], Loss: 0.0014, Acc: 57.9500%\n",
      "Epoch [64/100], Loss: 0.0012, Acc: 58.7400%\n",
      "Epoch [65/100], Loss: 0.0009, Acc: 58.5100%\n",
      "Epoch [66/100], Loss: 0.0008, Acc: 58.4800%\n",
      "Epoch [67/100], Loss: 0.0013, Acc: 58.4800%\n",
      "Epoch [68/100], Loss: 0.0011, Acc: 58.7700%\n",
      "Epoch [69/100], Loss: 0.0013, Acc: 59.7500%\n",
      "Epoch [70/100], Loss: 0.0022, Acc: 59.3000%\n",
      "Epoch [71/100], Loss: 0.0014, Acc: 59.1000%\n",
      "Epoch [72/100], Loss: 0.0019, Acc: 58.5900%\n",
      "Epoch [73/100], Loss: 0.0015, Acc: 59.1900%\n",
      "Epoch [74/100], Loss: 0.0013, Acc: 58.4700%\n",
      "Epoch [75/100], Loss: 0.0008, Acc: 58.7700%\n",
      "Epoch [76/100], Loss: 0.0010, Acc: 59.1600%\n",
      "Epoch [77/100], Loss: 0.0014, Acc: 59.0300%\n",
      "Epoch [78/100], Loss: 0.0007, Acc: 58.9900%\n",
      "Epoch [79/100], Loss: 0.0012, Acc: 59.3700%\n",
      "Epoch [80/100], Loss: 0.0017, Acc: 59.4600%\n",
      "Epoch [81/100], Loss: 0.0016, Acc: 59.1200%\n",
      "Epoch [82/100], Loss: 0.0018, Acc: 59.4700%\n",
      "Epoch [83/100], Loss: 0.0013, Acc: 59.5500%\n",
      "Epoch [84/100], Loss: 0.0014, Acc: 59.4000%\n",
      "Epoch [85/100], Loss: 0.0015, Acc: 59.6200%\n",
      "Epoch [86/100], Loss: 0.0009, Acc: 58.8400%\n",
      "Epoch [87/100], Loss: 0.0007, Acc: 59.3200%\n",
      "Epoch [88/100], Loss: 0.0017, Acc: 59.8200%\n",
      "Epoch [89/100], Loss: 0.0011, Acc: 59.6100%\n",
      "Epoch [90/100], Loss: 0.0006, Acc: 58.3400%\n",
      "Epoch [91/100], Loss: 0.0015, Acc: 59.4200%\n",
      "Epoch [92/100], Loss: 0.0014, Acc: 59.8400%\n",
      "Epoch [93/100], Loss: 0.0020, Acc: 58.8500%\n",
      "Epoch [94/100], Loss: 0.0013, Acc: 59.5800%\n",
      "Epoch [95/100], Loss: 0.0008, Acc: 60.0200%\n",
      "Epoch [96/100], Loss: 0.0012, Acc: 59.6400%\n",
      "Epoch [97/100], Loss: 0.0012, Acc: 59.2400%\n",
      "Epoch [98/100], Loss: 0.0008, Acc: 59.4800%\n",
      "Epoch [99/100], Loss: 0.0022, Acc: 59.7000%\n",
      "Epoch [100/100], Loss: 0.0012, Acc: 59.5800%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "# Traning the Model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for (images, labels) in train_loader:\n",
    "        images = images.to(device).view(-1, 3, 32, 32)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "        \n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if param.grad is not None:\n",
    "    #         print(f'{name} grad mean: {param.grad.mean()}')\n",
    "    # print(outputs)\n",
    "\n",
    "    # Calculate Accuracy         \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Predict test dataset\n",
    "    for images, labels in test_loader: \n",
    "        images = images.to(device).view(-1, 3, 32, 32)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        predicted = torch.max(outputs.data, 1)[1]\n",
    "        total += len(labels)\n",
    "        correct += (predicted == labels).sum()\n",
    "    \n",
    "    accuracy = 100 * correct / float(total)\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}, Acc: {accuracy:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "ac9e4aee-b8af-4641-8794-bad03b650179",
    "_uuid": "44c1ed412d778f3e6b08f11bddc5321f63e408dd"
   },
   "outputs": [],
   "source": [
    "# # visualization loss \n",
    "# plt.plot(iteration_list,loss_list)\n",
    "# plt.xlabel(\"Number of iteration\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"CNN: Loss vs Number of iteration\")\n",
    "# plt.show()\n",
    "\n",
    "# # visualization accuracy \n",
    "# plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
    "# plt.xlabel(\"Number of iteration\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.title(\"CNN: Accuracy vs Number of iteration\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 861823,
     "sourceId": 3004,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 18199,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "env_playing_with_dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
