{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "a0bf0fa7-c527-4fd3-b504-02a88fc94798",
    "_uuid": "1382c63fe24710d3b2840e7dcf172cddbf533743"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "59cdc9d5-da8f-4d7a-abc5-c62b0008afb0",
    "_uuid": "c6e0d7d3843719091564a580dbe08f67ee0d93ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 20\n",
    "\n",
    "# Transform: Normalize and Flatten Images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Lambda(lambda x: x.view(-1))  # Flatten the image\n",
    "])\n",
    "\n",
    "# Load CIFAR10 Dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "50bbb2e7-15d8-47f5-8f31-25e0d0cb9e29",
    "_uuid": "cd8f261d231acaccd0f0bc8466fc28c1b0c2f567"
   },
   "source": [
    "<a id=\"5\"></a> <br>\n",
    "### Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "9ca5af9e-6821-4d60-8084-edb523a39c6b",
    "_uuid": "4915535771ffdd33ef480200393216f215b4fc48"
   },
   "outputs": [],
   "source": [
    "# ðŸ”¥ Channel Attention (Squeeze-and-Excitation Block)\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=1):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)  # Global pooling\n",
    "        self.fc1 = nn.Linear(in_channels, in_channels // reduction)  # Reduce dim\n",
    "        self.fc2 = nn.Linear(in_channels // reduction, in_channels)  # Restore dim\n",
    "        self.sigmoid = nn.Sigmoid()  # Attention weights\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        out = self.global_avg_pool(x).view(b, c)  # Squeeze\n",
    "        out = F.relu(self.fc1(out))  # FC layer 1\n",
    "        out = self.sigmoid(self.fc2(out))  # FC layer 2 with Sigmoid\n",
    "        out = out.view(b, c, 1, 1)  # Reshape\n",
    "        return x * out  # Scale original feature maps\n",
    "\n",
    "# ðŸ”¥ Spatial Attention\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)  # 7x7 conv\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)  # Avg pooling across channels\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)  # Max pooling across channels\n",
    "        out = torch.cat([avg_out, max_out], dim=1)  # Concatenate channel-wise\n",
    "        out = self.conv(out)  # Apply Conv\n",
    "        return x * self.sigmoid(out)  # Apply attention weights\n",
    "\n",
    "\n",
    "# Create CNN Model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        # Convolutional layers with BatchNorm\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)  # BatchNorm for 32 feature maps\n",
    "        self.se1 = SEBlock(32)  # Channel Attention\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)  # BatchNorm for 64 feature maps\n",
    "        self.se2 = SEBlock(64)  # Channel Attention\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)  # BatchNorm for 128 feature maps\n",
    "        self.sa = SpatialAttention()  # Spatial Attention\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Downsampling\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(256)  # BatchNorm for FC layer\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 10)  # Output layer for 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # Conv -> BatchNorm -> ReLU -> Pool\n",
    "        x = self.se1(x)  # Apply Channel Attention\n",
    "\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.se2(x)  # Apply Channel Attention\n",
    "\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.sa(x)  # Apply Channel Attention\n",
    "\n",
    "        x = torch.flatten(x, 1)  # Flatten for FC layer\n",
    "        x = F.relu(self.bn_fc1(self.fc1(x)))  # FC -> BatchNorm -> ReLU\n",
    "        x = self.fc2(x)  # No activation for final layer (handled by loss function)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "model = CNNModel().to(device)\n",
    "error = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "99a8903c-da15-496c-96b7-f5402c8fc5f0",
    "_uuid": "f44e02d25698ac1a014795d972a384a3f3003d35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/25], Loss: 0.0011, Acc: 70.5500%\n",
      "Epoch [1/25], Loss: 0.0009, Acc: 74.6200%\n",
      "Epoch [2/25], Loss: 0.0005, Acc: 76.4900%\n",
      "Epoch [3/25], Loss: 0.0014, Acc: 77.8500%\n",
      "Epoch [4/25], Loss: 0.0003, Acc: 78.2300%\n",
      "Epoch [5/25], Loss: 0.0005, Acc: 78.3300%\n",
      "Epoch [6/25], Loss: 0.0005, Acc: 77.8700%\n",
      "Epoch [7/25], Loss: 0.0002, Acc: 78.0600%\n",
      "Epoch [8/25], Loss: 0.0003, Acc: 77.2400%\n",
      "Epoch [9/25], Loss: 0.0010, Acc: 77.8100%\n",
      "Epoch [10/25], Loss: 0.0004, Acc: 78.5900%\n",
      "Epoch [11/25], Loss: 0.0007, Acc: 78.0100%\n",
      "Epoch [12/25], Loss: 0.0002, Acc: 78.0800%\n",
      "Epoch [13/25], Loss: 0.0000, Acc: 78.0100%\n",
      "Epoch [14/25], Loss: 0.0001, Acc: 77.6400%\n",
      "Epoch [15/25], Loss: 0.0001, Acc: 77.6900%\n",
      "Epoch [16/25], Loss: 0.0001, Acc: 77.3000%\n",
      "Epoch [17/25], Loss: 0.0001, Acc: 77.7500%\n",
      "Epoch [18/25], Loss: 0.0003, Acc: 78.6100%\n",
      "Epoch [19/25], Loss: 0.0000, Acc: 77.7700%\n",
      "Epoch [20/25], Loss: 0.0001, Acc: 77.4900%\n",
      "Epoch [21/25], Loss: 0.0010, Acc: 77.1500%\n",
      "Epoch [22/25], Loss: 0.0000, Acc: 78.1800%\n",
      "Epoch [23/25], Loss: 0.0002, Acc: 77.9300%\n",
      "Epoch [24/25], Loss: 0.0000, Acc: 78.2600%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "\n",
    "# Traning the Model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for (images, labels) in train_loader:\n",
    "        images = images.to(device).view(-1, 3, 32, 32)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = error(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "        \n",
    "    # Calculate Accuracy         \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Predict test dataset\n",
    "    for images, labels in test_loader: \n",
    "        images = images.to(device).view(-1, 3, 32, 32)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        predicted = torch.max(outputs.data, 1)[1]\n",
    "        total += len(labels)\n",
    "        correct += (predicted == labels).sum()\n",
    "    \n",
    "    accuracy = 100 * correct / float(total)\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}], Loss: {avg_loss:.4f}, Acc: {accuracy:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "ac9e4aee-b8af-4641-8794-bad03b650179",
    "_uuid": "44c1ed412d778f3e6b08f11bddc5321f63e408dd"
   },
   "outputs": [],
   "source": [
    "# # visualization loss \n",
    "# plt.plot(iteration_list,loss_list)\n",
    "# plt.xlabel(\"Number of iteration\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"CNN: Loss vs Number of iteration\")\n",
    "# plt.show()\n",
    "\n",
    "# # visualization accuracy \n",
    "# plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
    "# plt.xlabel(\"Number of iteration\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.title(\"CNN: Accuracy vs Number of iteration\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 861823,
     "sourceId": 3004,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 18199,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "env_playing_with_dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
