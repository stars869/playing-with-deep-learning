{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "a0bf0fa7-c527-4fd3-b504-02a88fc94798",
    "_uuid": "1382c63fe24710d3b2840e7dcf172cddbf533743"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "59cdc9d5-da8f-4d7a-abc5-c62b0008afb0",
    "_uuid": "c6e0d7d3843719091564a580dbe08f67ee0d93ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 20\n",
    "\n",
    "# Transform: Normalize and Flatten Images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Lambda(lambda x: x.view(-1))  # Flatten the image\n",
    "])\n",
    "\n",
    "# Load CIFAR10 Dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "49344c72-d0ea-4092-96fe-ed5508ae6e0b",
    "_uuid": "20b4762eb8607ed428703c2156c5aefe8b49ff3f"
   },
   "source": [
    "<a id=\"3\"></a> <br>\n",
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "03a25584-c567-4b5e-bae1-7bc9e02184fe",
    "_uuid": "7c7a7265a23a8101d5ed0c8826dfec3726d6161d"
   },
   "outputs": [],
   "source": [
    "# Create Logistic Regression Model\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(32 * 32 * 3, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x.view(-1, 32 * 32 * 3))\n",
    "        return out\n",
    "\n",
    "\n",
    "model = LogisticRegressionModel().to(device)\n",
    "\n",
    "error = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "82de08d9-7f3c-4eb9-8a99-9d7a8677799c",
    "_uuid": "0cab9c3ec72f73db1b06578fa7a51611141e16da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/20], Loss: 0.0027, Acc: 29.5100%\n",
      "Epoch [1/20], Loss: 0.0026, Acc: 32.1100%\n",
      "Epoch [2/20], Loss: 0.0028, Acc: 33.3300%\n",
      "Epoch [3/20], Loss: 0.0024, Acc: 34.7900%\n",
      "Epoch [4/20], Loss: 0.0024, Acc: 35.3800%\n",
      "Epoch [5/20], Loss: 0.0022, Acc: 35.9000%\n",
      "Epoch [6/20], Loss: 0.0027, Acc: 36.0600%\n",
      "Epoch [7/20], Loss: 0.0025, Acc: 36.5900%\n",
      "Epoch [8/20], Loss: 0.0023, Acc: 36.2700%\n",
      "Epoch [9/20], Loss: 0.0021, Acc: 36.8800%\n",
      "Epoch [10/20], Loss: 0.0024, Acc: 36.4600%\n",
      "Epoch [11/20], Loss: 0.0019, Acc: 37.5400%\n",
      "Epoch [12/20], Loss: 0.0021, Acc: 37.5100%\n",
      "Epoch [13/20], Loss: 0.0031, Acc: 36.9700%\n",
      "Epoch [14/20], Loss: 0.0023, Acc: 37.8300%\n",
      "Epoch [15/20], Loss: 0.0022, Acc: 37.9300%\n",
      "Epoch [16/20], Loss: 0.0023, Acc: 37.8600%\n",
      "Epoch [17/20], Loss: 0.0018, Acc: 38.3800%\n",
      "Epoch [18/20], Loss: 0.0026, Acc: 38.5000%\n",
      "Epoch [19/20], Loss: 0.0028, Acc: 38.0500%\n"
     ]
    }
   ],
   "source": [
    "# Traning the Model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for (images, labels) in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = error(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "        \n",
    "    # Calculate Accuracy         \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Predict test dataset\n",
    "    for images, labels in test_loader: \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        predicted = torch.max(outputs.data, 1)[1]\n",
    "        total += len(labels)\n",
    "        correct += (predicted == labels).sum()\n",
    "    \n",
    "    accuracy = 100 * correct / float(total)\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}], Loss: {avg_loss:.4f}, Acc: {accuracy:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "924e9606-e155-4e39-89d3-39f941fd52f8",
    "_uuid": "db87c03e9d263f07eb75f82a914d3e966895a6c1"
   },
   "outputs": [],
   "source": [
    "# visualization\n",
    "# plt.plot(iteration_list,loss_list)\n",
    "# plt.xlabel(\"Number of iteration\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"Logistic Regression: Loss vs Number of iteration\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4d38db05-fad0-468c-9000-20caf5465eca",
    "_uuid": "ea9eba414f2f0f1e63ef564dc0ee708c753ff51f"
   },
   "source": [
    "<a id=\"4\"></a> <br>\n",
    "### Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "3472f1c1-5888-4abe-822c-3a493a5f8be5",
    "_uuid": "cefd0bb2f23b80f30ca65cbb08859ad81ab12e08"
   },
   "outputs": [],
   "source": [
    "# Create ANN Model\n",
    "class ANNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANNModel, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * 32 * 3, 128) \n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(32, 10)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = ANNModel().to(device)\n",
    "error = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.02\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "7550e98b-5011-4d09-88ee-97b0ecbc6f19",
    "_uuid": "c91694f3af94e4e1b76ab01489e186718c70ccd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Loss: 0.0024, Acc: 28.4600%\n",
      "Epoch [1/50], Loss: 0.0023, Acc: 30.6500%\n",
      "Epoch [2/50], Loss: 0.0026, Acc: 31.7600%\n",
      "Epoch [3/50], Loss: 0.0024, Acc: 36.2100%\n",
      "Epoch [4/50], Loss: 0.0023, Acc: 40.2300%\n",
      "Epoch [5/50], Loss: 0.0026, Acc: 42.5900%\n",
      "Epoch [6/50], Loss: 0.0018, Acc: 42.1700%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      6\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (images, labels) \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      8\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m32\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m32\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/env_playing_with_dp/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/env_playing_with_dp/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/env_playing_with_dp/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/env_playing_with_dp/lib/python3.12/site-packages/torchvision/datasets/cifar.py:119\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    116\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/anaconda3/envs/env_playing_with_dp/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/env_playing_with_dp/lib/python3.12/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mto_tensor(pic)\n",
      "File \u001b[0;32m~/anaconda3/envs/env_playing_with_dp/lib/python3.12/site-packages/torchvision/transforms/functional.py:176\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "# Traning the Model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for (images, labels) in train_loader:\n",
    "        images = images.view(-1, 3 * 32 * 32).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = error(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "        \n",
    "    # Calculate Accuracy         \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Predict test dataset\n",
    "    for images, labels in test_loader: \n",
    "        images = images.view(-1, 3 * 32 * 32).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        predicted = torch.max(outputs.data, 1)[1]\n",
    "        total += len(labels)\n",
    "        correct += (predicted == labels).sum()\n",
    "    \n",
    "    accuracy = 100 * correct / float(total)\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}], Loss: {avg_loss:.4f}, Acc: {accuracy:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "5579a7d6-7766-4d0f-b9d0-584cb4f28321",
    "_uuid": "c5e2e6da7f1ee801e38358dc28d4c99e32d2b761"
   },
   "outputs": [],
   "source": [
    "# # visualization loss \n",
    "# plt.plot(iteration_list,loss_list)\n",
    "# plt.xlabel(\"Number of iteration\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"ANN: Loss vs Number of iteration\")\n",
    "# plt.show()\n",
    "\n",
    "# # visualization accuracy \n",
    "# plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
    "# plt.xlabel(\"Number of iteration\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.title(\"ANN: Accuracy vs Number of iteration\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "50bbb2e7-15d8-47f5-8f31-25e0d0cb9e29",
    "_uuid": "cd8f261d231acaccd0f0bc8466fc28c1b0c2f567"
   },
   "source": [
    "<a id=\"5\"></a> <br>\n",
    "### Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_cell_guid": "9ca5af9e-6821-4d60-8084-edb523a39c6b",
    "_uuid": "4915535771ffdd33ef480200393216f215b4fc48"
   },
   "outputs": [],
   "source": [
    "# Create CNN Model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        # Convolutional layers with BatchNorm\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)  # BatchNorm for 32 feature maps\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)  # BatchNorm for 64 feature maps\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)  # BatchNorm for 128 feature maps\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Downsampling\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(256)  # BatchNorm for FC layer\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 10)  # Output layer for 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))  # Conv -> BatchNorm -> ReLU -> Pool\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        x = torch.flatten(x, 1)  # Flatten for FC layer\n",
    "        x = torch.relu(self.bn_fc1(self.fc1(x)))  # FC -> BatchNorm -> ReLU\n",
    "        x = self.fc2(x)  # No activation for final layer (handled by loss function)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "model = CNNModel().to(device)\n",
    "error = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_cell_guid": "99a8903c-da15-496c-96b7-f5402c8fc5f0",
    "_uuid": "f44e02d25698ac1a014795d972a384a3f3003d35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Loss: 0.0010, Acc: 69.7400%\n",
      "Epoch [1/50], Loss: 0.0011, Acc: 74.3500%\n",
      "Epoch [2/50], Loss: 0.0008, Acc: 76.2300%\n",
      "Epoch [3/50], Loss: 0.0005, Acc: 77.0800%\n",
      "Epoch [4/50], Loss: 0.0008, Acc: 77.9300%\n",
      "Epoch [5/50], Loss: 0.0004, Acc: 77.9400%\n",
      "Epoch [6/50], Loss: 0.0003, Acc: 77.6800%\n",
      "Epoch [7/50], Loss: 0.0004, Acc: 77.4900%\n",
      "Epoch [8/50], Loss: 0.0013, Acc: 77.6100%\n",
      "Epoch [9/50], Loss: 0.0005, Acc: 78.1000%\n",
      "Epoch [10/50], Loss: 0.0000, Acc: 77.9900%\n",
      "Epoch [11/50], Loss: 0.0006, Acc: 76.9700%\n",
      "Epoch [12/50], Loss: 0.0000, Acc: 77.6500%\n",
      "Epoch [13/50], Loss: 0.0002, Acc: 77.6400%\n",
      "Epoch [14/50], Loss: 0.0007, Acc: 77.9900%\n",
      "Epoch [15/50], Loss: 0.0011, Acc: 77.6900%\n",
      "Epoch [16/50], Loss: 0.0001, Acc: 78.0000%\n",
      "Epoch [17/50], Loss: 0.0001, Acc: 77.8400%\n",
      "Epoch [18/50], Loss: 0.0001, Acc: 77.3700%\n",
      "Epoch [19/50], Loss: 0.0000, Acc: 77.9000%\n",
      "Epoch [20/50], Loss: 0.0000, Acc: 78.1700%\n",
      "Epoch [21/50], Loss: 0.0003, Acc: 77.2900%\n",
      "Epoch [22/50], Loss: 0.0002, Acc: 78.2500%\n",
      "Epoch [23/50], Loss: 0.0003, Acc: 79.0300%\n",
      "Epoch [24/50], Loss: 0.0001, Acc: 77.3800%\n",
      "Epoch [25/50], Loss: 0.0004, Acc: 77.1500%\n",
      "Epoch [26/50], Loss: 0.0002, Acc: 78.0900%\n",
      "Epoch [27/50], Loss: 0.0000, Acc: 77.1200%\n",
      "Epoch [28/50], Loss: 0.0002, Acc: 78.2200%\n",
      "Epoch [29/50], Loss: 0.0005, Acc: 77.9200%\n",
      "Epoch [30/50], Loss: 0.0001, Acc: 78.2400%\n",
      "Epoch [31/50], Loss: 0.0001, Acc: 77.7100%\n",
      "Epoch [32/50], Loss: 0.0001, Acc: 78.3500%\n",
      "Epoch [33/50], Loss: 0.0003, Acc: 76.6600%\n",
      "Epoch [34/50], Loss: 0.0000, Acc: 78.0000%\n",
      "Epoch [35/50], Loss: 0.0001, Acc: 78.0300%\n",
      "Epoch [36/50], Loss: 0.0001, Acc: 77.8800%\n",
      "Epoch [37/50], Loss: 0.0000, Acc: 77.8100%\n",
      "Epoch [38/50], Loss: 0.0000, Acc: 78.1800%\n",
      "Epoch [39/50], Loss: 0.0004, Acc: 78.4200%\n",
      "Epoch [40/50], Loss: 0.0000, Acc: 78.1900%\n",
      "Epoch [41/50], Loss: 0.0001, Acc: 78.0600%\n",
      "Epoch [42/50], Loss: 0.0000, Acc: 77.4800%\n",
      "Epoch [43/50], Loss: 0.0000, Acc: 78.0400%\n",
      "Epoch [44/50], Loss: 0.0000, Acc: 78.4500%\n",
      "Epoch [45/50], Loss: 0.0005, Acc: 78.1100%\n",
      "Epoch [46/50], Loss: 0.0001, Acc: 77.3900%\n",
      "Epoch [47/50], Loss: 0.0000, Acc: 77.8600%\n",
      "Epoch [48/50], Loss: 0.0000, Acc: 78.1900%\n",
      "Epoch [49/50], Loss: 0.0000, Acc: 78.0400%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "\n",
    "# Traning the Model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for (images, labels) in train_loader:\n",
    "        images = images.to(device).view(-1, 3, 32, 32)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = error(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "        \n",
    "    # Calculate Accuracy         \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Predict test dataset\n",
    "    for images, labels in test_loader: \n",
    "        images = images.to(device).view(-1, 3, 32, 32)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        predicted = torch.max(outputs.data, 1)[1]\n",
    "        total += len(labels)\n",
    "        correct += (predicted == labels).sum()\n",
    "    \n",
    "    accuracy = 100 * correct / float(total)\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}], Loss: {avg_loss:.4f}, Acc: {accuracy:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "ac9e4aee-b8af-4641-8794-bad03b650179",
    "_uuid": "44c1ed412d778f3e6b08f11bddc5321f63e408dd"
   },
   "outputs": [],
   "source": [
    "# # visualization loss \n",
    "# plt.plot(iteration_list,loss_list)\n",
    "# plt.xlabel(\"Number of iteration\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"CNN: Loss vs Number of iteration\")\n",
    "# plt.show()\n",
    "\n",
    "# # visualization accuracy \n",
    "# plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
    "# plt.xlabel(\"Number of iteration\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.title(\"CNN: Accuracy vs Number of iteration\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 861823,
     "sourceId": 3004,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 18199,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "env_playing_with_dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
